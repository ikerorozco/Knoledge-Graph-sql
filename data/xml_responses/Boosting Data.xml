<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation</title>
				<funder ref="#_GV4nbWW">
					<orgName type="full">China NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinda</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuhao</forename><surname>Song</surname></persName>
							<email>songyuhao@haomo.ai</email>
							<affiliation key="aff1">
								<orgName type="laboratory">HAOMO</orgName>
								<orgName type="institution">AI Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daming</forename><surname>Wang</surname></persName>
							<email>wangdaming@haomo.ai</email>
							<affiliation key="aff1">
								<orgName type="laboratory">HAOMO</orgName>
								<orgName type="institution">AI Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weiwei</forename><surname>Zhao</surname></persName>
							<email>zhaoweiwei@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Minghua</forename><surname>Chen</surname></persName>
							<email>chenminghua@haomo.ai</email>
							<affiliation key="aff1">
								<orgName type="laboratory">HAOMO</orgName>
								<orgName type="institution">AI Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kangliang</forename><surname>Chen</surname></persName>
							<email>chenkangliang@haomo.ai</email>
						</author>
						<author>
							<persName><forename type="first">Qinya</forename><surname>Li</surname></persName>
							<email>qinyali@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">HAOMO</orgName>
								<orgName type="institution">AI Technology Co., Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quality over Quantity: Boosting Data Efficiency Through Ensembled Multimodal Data Curation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BE5B0CA763AB0C29C311C47E9A6B2754</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-05-20T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=0, consolidateHeader=0, consolidateFunders=0, includeRawAffiliations=false, includeRawCitations=false, includeRawCopyrights=false, generateTeiIds=false, generateTeiCoordinates=[], sentenceSegmentation=false, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In an era overwhelmed by vast amounts of data, the effective curation of web-crawl datasets is essential for optimizing model performance. This paper tackles the challenges associated with the unstructured and heterogeneous nature of such datasets. Traditional heuristic curation methods often inadequately capture complex features, resulting in biases and the exclusion of relevant data. We introduce an advanced, learning-driven approach, Ensemble Curation Of DAta ThroUgh Multimodal Operators (EcoDatum), incorporating a novel quality-guided deduplication method to ensure balanced feature distributions. EcoDatum strategically integrates various unimodal and multimodal data curation operators within a weak supervision ensemble framework, utilizing automated optimization to score each data point effectively. EcoDatum, which significantly improves the data curation quality and efficiency, outperforms existing state-of-theart (SOTA) techniques, ranked 1 st on the DataComp leaderboard, with an average performance score of 0.182 across 38 diverse evaluation datasets. This represents a 28% improvement over the DataComp baseline method, demonstrating its effectiveness in improving dataset curation and model training efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The vast amount of data presents opportunities for training advanced deep learning models, but it also introduces significant noise and irrelevant information, which can hinder model effectiveness. In both academia and industry, the need for robust data curation techniques to extract meaningful signals from extensive digital information has become a pressing concern. In web-crawled datasets, data curation is a multi-faceted task involving various stages and methodologies. The core objective is to identify and retain high-quality samples while discarding noise or mitigating the impact of irrelevant data <ref type="bibr" target="#b13">(Hoffmann et al. 2022</ref>). This process is crucial for optimizing model performance in the deep learning framework.</p><p>Figure <ref type="figure">1</ref>: Web-crawled image-text datasets often vary in quality. EcoDatum addresses this by integrating unimodal and multimodal operators to discriminate data quality, guiding the selection process to enhance data curation efficiency.</p><p>Web-crawled data is inherently unstructured, diverse, and constantly evolving, making it essential to develop adaptive curation methods capable of handling such complexity <ref type="bibr" target="#b13">(Hoffmann et al. 2022)</ref>. Traditionally, data curation methods have relied heavily on heuristic filtering approaches based on manually defined content attributes, such as image resolution, graphic geometry, textual length, and linguistic complexity. While these heuristic methods provide a basic means of recognizing low-quality samples, they fail to adequately capture the subtle features of web-crawled data and may introduce biases or overlook relevant information <ref type="bibr" target="#b22">(Mindermann et al. 2022;</ref><ref type="bibr" target="#b21">Maini, Yaghini, and Papernot 2021)</ref>. To address these limitations, researchers are increasingly adopting automated curation methods that leverage deep learning techniques, including natural language processing, computer vision and cross-modal representation learning, to achieve a balance of quality and quantity. <ref type="bibr" target="#b29">(Schuhmann et al. 2021;</ref><ref type="bibr" target="#b5">Brown 2020;</ref><ref type="bibr" target="#b31">Torkashvand, Jameii, and Reza 2023)</ref>.</p><p>This research proposes a data curation framework called EcoDatum to address the aforementioned issues. Specifi-cally, we implement a range of efficient data curation strategies as operators, to enhance the data curation process and to achieve cross-modal data alignment at various levels of granularity, as detailed in Figure <ref type="figure">1</ref>. However, a simple combination of these operators may introduce bias and lead to insufficient utilization of their individual strengths. To fully capture their synergies, we develop a weak supervision ensemble framework that integrates these operators, achieving a synergistic effect. Furthermore, to enhance the integration efficiency of unimodal and multimodal operators, EcoDatum introduces an automated optimization approach. This is achieved by tuning the weak supervision integration module using a composite metric and a tiny-labeled dataset.</p><p>As a novel weak supervision-based framework for multimodal data curation, EcoDatum achieves state-of-the-art performance on the DataComp data filtering track <ref type="bibr" target="#b12">(Gadre et al. 2024)</ref>. The visual language model trained on curated data demonstrates outstanding results over 38 downstream tasks, highlighting its strong generalizability. Extended experiments demonstrate the effectiveness of this research in understanding various operators in cross-modal data management, offering insights for future work.</p><p>Our main contributions are as follows:</p><p>1. We propose an auto-optimized ensemble framework, EcoDatum, which integrates techniques to enhance data quality and curate multimodal data, ensuring aligned, high-quality inputs for visual language pretraining. 2. We introduce a search-based optimization algorithm for weak supervision labeling function tuning that enhances the curation process and boosts the system's robustness. 3. EcoDatum surpasses existing state-of-the-art techniques in the DataComp benchmark over 38 downstream tasks and ranks 1 st on the leaderboard<ref type="foot" target="#foot_0">foot_0</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Data Curation for Web-crawled Datasets</p><p>Recent research underscores the critical role of data curation in enhancing model performance with large-scale image-text datasets. Various studies focus on improving dataset quality through curation methods, such as enhancing the descriptiveness and cross-modal feature alignment of imagetext pairs, and reducing redundancy <ref type="bibr" target="#b24">(Radenovic et al. 2023;</ref><ref type="bibr" target="#b23">Nguyen et al. 2024;</ref><ref type="bibr" target="#b0">Abbas et al. 2023)</ref>.</p><p>In a broader context, DataComp is a benchmark designed to evaluate the performance of multimodal models on largescale, real-world datasets <ref type="bibr" target="#b11">(Gadre et al. 2023)</ref>. Recent advancements in the DataComp benchmark highlight notable progress in data curation techniques. Yokoo et al. <ref type="bibr" target="#b33">(Yokoo et al. 2023</ref>) advanced data filtering using image-text similarity and caption modification, achieving notable progress in the Filtering and Bring Your Own Data (BYOD) Tracks. <ref type="bibr" target="#b34">Yu et al.(Yu et al. 2023</ref>) evaluated data selection criteria's impact on model performance, while Chen et al. <ref type="bibr" target="#b6">(Chen et al. 2024)</ref> introduced DataJuicer for managing large-scale datasets. <ref type="bibr" target="#b23">Nguyen et al.(Nguyen et al. 2024</ref>) enhanced image captioning for better modality alignment, and Maini et al. <ref type="bibr" target="#b20">(Maini et al. 2023)</ref> presented T-MARS for improved visual representation by bypassing text feature learning. Additionally, significant contributions have been made in the areas of synthetic data, contrastive learning, image-text alignment, and few-shot learning <ref type="bibr" target="#b7">(Chen et al. 2020;</ref><ref type="bibr" target="#b25">Radford et al. 2021;</ref><ref type="bibr" target="#b14">Jia et al. 2021;</ref><ref type="bibr" target="#b16">Li et al. 2022;</ref><ref type="bibr" target="#b1">Alayrac et al. 2022)</ref>.</p><p>Despite the significant advancements in data curation achieved by <ref type="bibr" target="#b24">Radenovic et al.(Radenovic et al. 2023)</ref> and <ref type="bibr" target="#b23">Nguyen et al.(Nguyen et al. 2024</ref>) through enhancing data relevance, existing automated filtering methods may still exclude valuable but less conventional data points or introduce biases by focusing too narrowly on specific aspects, potentially overlooking broader contextual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ensemble Learning</head><p>Ensemble learning, which combines multiple models to improve performance and generalization, has long been a foundational approach in machine learning. Classic methods such as Bagging <ref type="bibr" target="#b4">(Breiman 1996)</ref> and Boosting <ref type="bibr" target="#b10">(Freund and Schapire 1997)</ref> initially demonstrated how model aggregation could reduce variance and improve accuracy.</p><p>Afterwards, ensemble learning has been increasingly applied to specialized tasks. Zimek, Schubert, &amp; Kriegel (Zimek, Schubert, and Kriegel 2012) highlighted how ensemble methods enhance outlier detection by aggregating results from multiple models. <ref type="bibr" target="#b3">Beluch et al.(Beluch et al. 2018</ref>) demonstrated that ensemble-based uncertainty sampling can significantly improve efficiency in active learning. Rasmus et al. <ref type="bibr" target="#b26">(Rasmus et al. 2015)</ref> demonstrated that ensemble techniques enhance semi-supervised learning by effectively utilizing both labeled and unlabeled data. <ref type="bibr" target="#b30">Song et al.(Song et al. 2022)</ref> used ensemble methods to improve data quality by detecting and filtering noisy or mislabeled data.</p><p>These advancements illustrate how ensemble techniques refine data preprocessing and improve model inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Overview</head><p>As illustrated in Figure <ref type="figure">2</ref>, EcoDatum enhances the pretraining effectiveness of multimodal models like CLIP <ref type="bibr" target="#b25">(Radford et al. 2021</ref>) by strategically selecting high-quality subset Ŝ from the original dataset S. This targeted data curation improves the model's zero-shot performance on diverse tasks. The framework utilizes an ensemble of specialized data operators for comprehensive quality assessment, which addresses various dimensions including image filtering, text analysis, and cross-modal alignment at multiple granular levels. Automated optimization enables the weak supervision system to generate quality scores for data samples, thus minimizing manual input and enhancing the precision of threshold settings. Consequently, EcoDatum streamlines the data curation process and significantly elevates the quality, ensuring the dataset meets the rigorous requirements for model training.</p><p>Figure <ref type="figure">2</ref>: Overview of the EcoDatum Framework. EcoDatum utilizes quality-guided deduplication along with an ensemble of unimodal and multimodal data curation operators, that strategically curate multimodal datasets. This integrated approach systematically scores each data point, ensuring optimal quality and alignment for effective visual-language pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Guided Deduplication</head><p>To improve our dataset's diversity and distribution, we employ a quality-guided deduplication process that removes redundant text-image pairs. This approach uses perceptual hashing <ref type="bibr" target="#b9">(Farid 2021)</ref> to generate hash codes, identifying duplicates based on visual and textual content. Subsequently, the CLIP model assesses the semantic coherence of each duplicate group, allowing us to retain text-image pairs with the highest CLIP scores, as shown in Figure <ref type="figure">3</ref>. This selective retention enhances the dataset by preserving the most relevant and semantically rich examples, minimizing redundancy while maintaining quality and diversity.</p><p>Figure <ref type="figure">3</ref>: Quality Guided Deduplication retains the samples with better cross-modal alignment in duplicate groups to enhance the overall quality and achieve optimal data distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unimodal and Multimodal Curation Operators</head><p>EcoDatum enhances the quality of multimodal datasets by implementing rigorous unimodal and multimodal curation operators. The unimodal curation operators systematically filter out low-quality visuals and evaluate textual data for concreteness and relevance using both language identification and Image Caption Concreteness (ICC) metric <ref type="bibr" target="#b32">(Yanuka et al. 2024)</ref>. Multimodal curation integrates these approaches with advanced alignment techniques, employing models like GroundingDINO, an advanced open-set object detector <ref type="bibr" target="#b19">(Liu et al. 2023)</ref> for precise local feature alignment and the CLIP model, for global semantic coherence. Together, these strategies ensure the curated dataset is of high quality, with well-aligned multimodal content.</p><p>Unimodal Curation Operators. For images, the specific heuristic operators filter out blurred and low-quality visuals. For texts, the FastText <ref type="bibr" target="#b15">(Joulin et al. 2016</ref>) model identifies the language and the ICC metric evaluates the relevance and clarity of textual data using a pre-trained autoencoder.</p><p>Image-based quality filtering. Low-quality images can severely impact the learning of visual semantics. Our unimodal operators, based on heuristic rules, enhance dataset quality by filtering out images with detrimental attributes. The Geometric Operator targets images with non-standard aspect ratios that distort geometric relationships and compromise visual integrity when resized. Additionally, the Dat-aComp dataset contains many intentionally blurred images to meet privacy standards, which reduces the visual detail crucial for effective model training. The Blurry Operator identifies and removes these excessively blurred images, ensuring that the curated dataset retains high visual quality.</p><p>Text-based caption assessment. We leverage the FastText model to identify and remove captions in rare languages, enhancing the linguistic consistency of our dataset. Additionally, we use the ICC metric, developed by a pre-trained autoencoder, to independently assess and filter captions. Eco-Datum ensures the dataset retains only concrete and relevant captions, directly corresponding to their images.</p><p>Multimodal Curation Operators. EcoDatum enhances multimodal data curation by integrating both global and local image-text features, as shown in Figure <ref type="figure" target="#fig_0">4</ref>. We employ GroundingDINO for precise local feature alignment, ensuring detailed correspondence between text and images at the object level. Additionally, we utilize the CLIP model, augmented with innovative adaptations, to maintain global semantic coherence throughout the dataset.</p><p>Local Cross-Modal Feature Alignment. We utilize GroundingDINO for the precise alignment of text descriptions with corresponding visual content. It integrates and analyzes text and visual data, effectively identifying relevant phrases in captions and accurately localizing associated visual elements within images, ensuring precise text-to-image alignment without prompt modification.</p><p>To quantitatively assess the alignment between text and images, we develop a metric based on the count of bounding boxes with confidence scores exceeding a predefined thresh- old, as shown in Eq (1). This metric serves to highlight the degree of correspondence between textual descriptions and visual representations. A higher count of accurate detections indicates richer, more detailed scenes, signifying that these data points are of higher value for training and subsequent applications. Data points that do not meet this threshold can be effectively filtered out, including those where the described objects do not visually correspond to the images or where the textual descriptions are insufficiently specific. This ensures our dataset excludes mismatches and generalities, retaining only high-quality, relevant multimodal content.</p><formula xml:id="formula_0">Count GroundingDINO = n i=1 {x i &gt; t} (1)</formula><p>where x i represents the confidence score of the i-th detected object, n represents the total number of objects detected, and t represents the predefined threshold. This operator enhances the ability to curate multimodal data effectively, ensuring that the dataset maintains the most relevant and accurately aligned text-image pairs locally.</p><p>Global Cross-Modal Feature Alignment. In this module, EcoDatum utilizes the CLIP model, celebrated for its ability to assess the global semantic similarity between text descriptions and their visual counterparts. However, the effectiveness of the CLIP-Score can be compromised when images contain textual content that overlaps with captions. This issue is observed in 40% of the LAION dataset <ref type="bibr" target="#b28">(Schuhmann et al. 2022</ref>) and 20% of the Datacomp dataset <ref type="bibr" target="#b20">(Maini et al. 2023)</ref>. To mitigate this, we implement an innovative adaptation known as Flip-CLIP, which includes Horizontal-CLIP (H-CLIP) and Vertical-CLIP (V-CLIP) techniques, inspired by <ref type="bibr" target="#b34">(Yu et al. 2023)</ref>. Before computing the CLIP scores, images are flipped horizontally or vertically, reducing the model's bias towards text-based features and enabling more equitable evaluations of purely visual elements. The development of Flip-CLIP is motivated by the observation that OCR tasks often disproportionately influence the standard CLIP score, especially when the image-text is overlapped.</p><p>By integrating both CLIP-Score and Flip-CLIP-Score, we foster the model's ability to learn from visual content independently of textual influences, thereby enhancing Eco-Datum's capability to process and understand global visual features without excessive bias towards textual elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality Operators Ensemble</head><p>Given the vast volume of data and the high cost associated with obtaining high-quality labeled data, the availability of reliable labels is often limited. EcoDatum introduces a weak supervision labeling system that allows the efficient generation of quality-indicated labels at scale, mitigating the challenges of data scarcity and ensuring a more robust data quality assessment. In this study, data curation is abstracted as a data quality discrimination task, aiming to identify "highquality" data. This ensemble-based system further enhances the capabilities of the data operators described above.</p><p>Specifically, EcoDatum employs a weak supervision ensemble model called LabelModel <ref type="bibr" target="#b27">(Ratner et al. 2017;</ref><ref type="bibr" target="#b2">Bach et al. 2019</ref>) into the scope of data curation research, which integrates signal sources abstracted from unimodal and multimodal operators for data quality evaluation. This integration balances the limitations of individual operators and significantly reduces their erroneous impacts.</p><p>Each operator serves as an independent weak supervision signal source, assessing data quality from its unique dimension. The integration approach in this work uses LabelModel to combine multiple operators, automatically inferring a data quality score for each data sample by modeling the accuracy and relationships of these operators.</p><p>This process begins by matching each operator with corresponding labeling functions (LFs) <ref type="bibr" target="#b27">(Ratner et al. 2017)</ref>, which converts the operator's inferred score s of the data sample x i into weak supervision label L, as shown in Eq (2). The LFs compute operators' inference results with the mean value b and the standard deviation β of the decision boundary to transform continuous scores into discrete labels. These labels are then aggregated to form a comprehensive weak supervision label matrix. In this context, weak supervision labeling with "Abstain" addresses situations where LFs face unclear features or inapplicable rules. Allowing the LabelModel to abstain from assigning labels in these cases prevents the generation of incorrect labels. This approach enhances the LabelModel's ability to integrate diverse LFs by learning transformed matrix, particularly when they exhibit different biases and error patterns, thereby increasing the model's robustness when handling heterogeneous data.</p><formula xml:id="formula_1">L xij =    1, if s xij ≥ b j + β j (Selected) 0, if s xij ≤ b j -β j (Filtered) -1, if b j -β j &lt; s xij &lt; b j + β j (Abstain)</formula><p>(2) The LabelModel learns the transformed weak supervision label matrix L M , estimating the weight w j for each LF. These weights are used to combine the outputs of all LFs, ultimately generating a score for each data sample, which determines whether it is retained or filtered out. This approach enhances the comprehensiveness and robustness Figure <ref type="figure">5</ref>: Overview of Search-Based LFs Combinations Optimization. This method optimizes LFs to create a more accurate weak supervision label matrix. The approach involves automatically searching optimal LFs to improve the matrix used by the LabelModel, evaluating LF combinations using a specialized composite metric over a gathered tiny-labeled dataset. This process boosts the LabelModel's ability to assess data quality by refining operator interrelations and minimizing manual adjustments. of data quality evaluation, which ultimately allows the La-belModel to score all raw data and reflect quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search-based Optimization</head><p>A novel search-based optimization method is introduced to enhance the design of LFs, improving the generation of a more accurate weak supervision label matrix for La-belModel modeling, as shown in Figure <ref type="figure">5</ref>. This method addresses the challenge of converting operator-derived scores into labels by automatically optimizing LFs, reducing the need for manual experimentation. To further optimize the performance of the ensemble, EcoDatum proposes a composite metric that integrates the LabelModel's data quality assessment capability with the attributes of LFs combination from the transformation steps, enabling a refined weak supervision label matrix. This approach enhances the LabelModel's ability to analyze operator interrelations and importance, producing quality scores for data samples that closely approximate the ideal.</p><p>The evaluation stage automatically constructs a small labeled dataset containing "clean" and "noisy" samples. Clean data, labeled "1", are sourced from the COCO dataset <ref type="bibr" target="#b17">(Lin et al. 2014)</ref>, while "noisy" samples, labeled "0", are randomly sampled from the DataComp dataset to introduce both unimodal and multimodal noise and include added cross-modal noise through image-text pair exchanges. This setup tests the LabelModel's ability to differentiate data quality via the F1-tiny scores in Eq (3). Importantly, this dataset is only used for assessing the LabelModel's performance and does not contribute to training the model or optimizing Eq (3) coefficients, ensuring unbiased validation of the LF effectiveness.</p><p>To evaluate the data quality discrimination capacity of the LabelModel after learning generated weak supervision label matrics with different combinations of LFs, this research develops a specialized composite metric, shown in Eq (3), which combines classification metrics against ground truth and further incorporates the attributes of each opera- end if 13: end for 14: return LF s * tors' LFs, specifically measuring the f Overlap , f Conflict , and f Coverage . Here, they respectively indicate the frequency of agreement among LFs, the extent of disagreements, and the proportion of data labeled by at least one function.</p><formula xml:id="formula_2">M = α 1 •F 1 tiny +α 2 •f Overlap -α 3 •f Conflict +α 4 •f Coverage (3)</formula><p>Here, α 1 , α 2 , α 3 , α 4 are coefficients that are determined through a few rounds of experiments. These coefficients are tuned to optimize the balance between classification performance on the tiny labeled dataset and the contributions from overlap, conflict, and coverage metrics within the weak supervision labeling framework.</p><formula xml:id="formula_3">f Overlap = 1 n n i=1 I   m j=1 I(LF j (x i ) ̸ = 0) &gt; 1   (4) f Conflict = 1 n n i=1 I (∃ j 1 ̸ = j 2 ,LF j1 (x i ) ̸ = LF j2 (x i ) ̸ = 0)</formula><p>(5)</p><formula xml:id="formula_4">f Coverage = 1 n n i=1 I   m j=1 I(LF j (x i ) ̸ = 0) ≥ 1   (6)</formula><p>These metrics capture the model's effectiveness in integrating weak supervision signals. For example, even if the model achieves a high F1-score F 1 tiny on a tiny labeled dataset, significant conflicts between LFs or low coverage might still lead to instability in real-world applications. By optimizing these composite metrics, we can enhance the model's generalization ability across different datasets.</p><p>The overall optimization process involves repeatedly constructing the LabelModel across candidate LFs combinations and using the results of the aforementioned composite metrics to identify the optimal LabelModel leaned by each weak supervision label matrix, as shown in Algorithm 1. This optimized model is then applied to the final data curation task. By employing the optimized LabelModel, the overall framework can maximize the robustness of data quality assessment, balance the limitations of individual operators, and ultimately enhance overall data efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Dataset and Benchmark</head><p>The DataComp benchmark uniquely emphasizes data curation over model development. Unlike typical machine learning competitions that seek the best model with a fixed dataset, DataComp challenges participants to curate optimal datasets using fixed training code. This highlights the crucial role of high-quality, well-curated data in enhancing model performance. We choose the small-scale filtering track to validate the proposed framework EcoDatum, we curate a subset from a vast pool of 12.8 million image-text pairs from Common Crawl, adhering to the competition's constraints of fixed training parameters and computational budgets. Our objective is to efficiently filter this dataset, ensuring consistency in training iterations regardless of dataset size.</p><p>The effectiveness of our curated dataset is evaluated across 38 diverse datasets, including ImageNet, 6 ImageNet distribution shift datasets, 12 datasets from the Visual Task Adaptation Benchmark, three retrieval datasets, and several others <ref type="bibr" target="#b12">(Gadre et al. 2024)</ref>. This extensive range of evaluation datasets tested the generalizability and robustness of EcoDatum, providing a comprehensive assessment of their impact on model training across various real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>For the local cross-modal curation operator, we employ the GroundingDINO-based model <ref type="bibr" target="#b18">(Liu et al. 2025)</ref> with Swin-Large as the image backbone and BERT-Base <ref type="bibr" target="#b8">(Devlin 2018)</ref> for encoding text, setting confidence thresholds at 0.1 to retain more potentially feature-aligned data. In global crossmodal curation, we use the CLIP-ViT-Large-14 architecture <ref type="bibr" target="#b25">(Radford et al. 2021)</ref>. In determining final data volume, we conducted extensive experiments and reviewed related works, concluding that approximately the top 40% samples by the EcoDatum quality score after deduplication (around 3.5M) provide the best balance between quality and quantity. Experiments utilized 8 NVIDIA A100 GPUs, The training and evaluation process required 2.5 hours. Data curation for the 12.8 million dataset involved approximately 10 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Result Analysis</head><p>Existing Baselines. Several SOTA methods have previously set benchmarks in data filtering. LAION and CLIP Score utilize the CLIP model to refine datasets, while Datacomp Filtering employs heuristic unimodal operators for targeted data refinement <ref type="bibr" target="#b11">(Gadre et al. 2023</ref>). The HYPerbolic Entailment (HYPE) Filtering technique <ref type="bibr" target="#b15">(Kim et al. 2024</ref>) enhances data quality by integrating unimodal specificity with crossmodal alignment. LINE's strategy leverages large models for web data curation <ref type="bibr" target="#b33">(Yokoo et al. 2023</ref>). The Text-Masking and Re-Scoring (T-MARS) method corrects imbalances where textual features overpower visual ones <ref type="bibr" target="#b20">(Maini et al. 2023)</ref>, and the University of Wisconsin-Madison's (WS) approach utilizes an ensemble of object detection methods to optimize data filtering <ref type="bibr" target="#b14">(Huang et al. 2024)</ref>.</p><p>Performance Comparison. Building upon these foundations, EcoDatum enhances both efficiency and model training outcomes. As outlined in Table <ref type="table" target="#tab_0">1</ref>, using only 3.5 million data pairs from the original 12.8 million, EcoDatum achieved the highest average score of 0.182. This surpasses the performance of established methods like T-MARS and WS, both of which scored 0.180 across 38 diverse evaluation datasets. This curation strategy not only reduces computational overhead by 72% but also significantly improves data quality. EcoDatum exceeds the "No Filtering" baseline score of 0.132 and the Datacomp Basic filtering score of 0.142 by 28%. The integration of advanced methodologies like our optimized LabelModel for labeling functions tuning further refines the data curation process, setting new benchmarks in multimodal applications. The empirical results robustly validate our hypothesis that smaller, wellcurated datasets can outperform larger, unfiltered datasets, underscoring the effectiveness of EcoDatum. Moreover, additional experiments show that EcoDatum consistently improves performance and scales effectively with increasing dataset size.</p><p>In this study, we introduce a composite metric designed to automatically optimize the generation of labeling functions (LFs), thereby facilitating the creation of a more accurate weak supervision label matrix. This optimization directly enhances the learning efficiency of the LabelModel, significantly improving its ability to assess data quality. To validate the effectiveness of this composite metric, we conducted a rigorous experimental case study. The process involved documenting a systematic search to identify the most effective LF combinations and repeatedly evaluating their impact on the average performance across a diverse set of 38 bench- mark tasks. The results, depicted in Figure <ref type="figure" target="#fig_2">6</ref>, demonstrate a consistent positive correlation between the composite metric scores and the model's performance, affirming the metric's utility in refining the data curation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>This experiment conducts a systematic evaluation of data filtering techniques to assess impacts on the performance of the deep learning model, as detailed in Table 2: Performance comparison of different data curation and ensemble techniques over 38 downtasks.</p><p>The introduction of QGD achieves a 1.4% improvement over the random method with the same dataset size. Incorporating a unimodal operators' ensemble within the QGD framework results in a 4.8% improvement, while a multimodal operators' ensemble leads to a more substantial 9.5% enhancement. These results highlight the efficacy of both unimodal and multimodal operator ensembles in data curation. By integrating QGD with both unimodal and multimodal ensembles, the combined approach outperforms all others, showing a 45.4% improvement in performance compared to the "No Filtering" baseline. These experiments illustrate that EcoDatum strategically integrates advanced deduplication techniques and sophisticated ensemble frameworks to markedly elevate data quality, optimizing the pretraining process for multimodal models.</p><p>We conduct another ablation study to assess the individual contributions of data processing operators in data curation. By applying each operator independently and incrementally adding them, we explored their impact on downstream tasks. This approach allowed us to identify the most effective combinations of operators, significantly streamlining the optimization process. Through meticulous integration and refinement of labeling function (LF) constructions, we deter-mined the most efficient operator combinations, thereby enhancing the accuracy and efficacy of our data curation methods. This conclusion suggests a strategic approach when dealing with massive web data and limited computational resources: focusing on alignment techniques can lead to more efficient data filtering. Such a focus can improve the generalization performance of multimodal models. Potentially, this experiment could pave the way for future research, indicating that more advanced image-text matching techniques might result in even better multimodal curation outcomes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>The volume of web-crawled datasets is rapidly expanding, and training multimodal models with such data are increasingly prevalent. This paper addresses the challenge of variable sample quality in web-crawled datasets by introducing a novel data curation framework, EcoDatum, designed to select high-quality data. EcoDatum begins with quality-guided deduplication to preprocess the data, followed by the integration of unimodal and multimodal operators into a weak supervision ensemble model, LabelModel, and have employed a search-based optimization method to refine the labeling matrix within LabelModel. Our experiments demonstrate robust performance across all evaluated tasks, securing a 1 st place ranking in the small-scale track of the Dat-aComp benchmark. While this study validated EcoDatum on a small dataset, future work will extend the evaluation to larger datasets. This expansion will further test the scalability of EcoDatum, aiming to solidify its effectiveness and efficiency in enhancing the training of multimodal models with diverse, large-scale web-crawled data.  A.2.Single Operator Curations' Results. We conduct single-operator filtering experiments on the entire dataset to better utilize data curation operators and design a more efficient integration. In these experiments, we do not standardize the amount of data selected by each operator. Instead, to maximize the reflection of each operator's unique characteristics, we use their default settings for data selection (partially referencing related works, such as DataComp).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Dataset Size Avg. Perf. The results shown in Table <ref type="table" target="#tab_3">3</ref> demonstrate that the global feature alignment operator achieved the best single-operator performance on the same benchmark. This aligns with the findings from the progressive integration experiments discussed in the main text, reinforcing the conclusion that the global feature alignment operator makes a significant contribution to image-text data filtering tasks.</p><p>Additionally, by using a single operator independently, we observed that data curation with just one operator introduces certain biases. This bias arises because each operator evaluates data samples from a different perspective. Therefore, this finding highlights the importance and value of effectively utilizing multiple operators in a complementary manner. A.3.Quality Guided Deduplication (QGD) Visualization.</p><p>In the context of multimodal image-text datasets, traditional unimodal deduplication methods that randomly remove duplicates are insufficient. These unimodal methods often overlook the crucial interplay between image and text, leading to potential misalignments in retained data. Random selection introduces uncertainty regarding the degree of coherence between the two modalities, which can compromise the multimodal dataset's utility.</p><p>In the EcoDatum approach, deduplication takes into account the quality of image-text alignment. For duplicate image-text pairs, as shown in Figure <ref type="figure" target="#fig_5">8</ref>, rather than randomly selecting which to keep, the process identifies and retains the pair with the best global alignment. This method ensures that the highest quality and most relevant examples are preserved. It significantly improves the overall data quality and distribution, enhancing the dataset's diversity and reducing redundancy without sacrificing alignment accuracy.</p><p>QGD is particularly beneficial because it: • Preserves Semantic Integrity: Ensure that the retained data is the most representative in terms of both visual and textual accuracy.  Thus, moving away from random deduplication to a more deliberate, quality-guided approach significantly bolsters the effectiveness and reliability of multimodal datasets. A.4.Comparison With Different Global Feature Alignment Strategies. For certain horizontally symmetrical letters, simple horizontal flipping may fail to distinguish them, leading to mismatched image-text pairs being incorrectly judged as matching, thereby introducing noise. The global feature alignment operator in EcoDatum addresses this by performing flips in both directions, ensuring that the impact of symmetrical text in images on image-text similarity calculations is fully considered.</p><p>Through visualized results as Figure <ref type="figure" target="#fig_7">9</ref>, it was observed that for images containing symmetrical letters, even when both CLIP and H-CLIP fail to accurately assess the quality, the introduction of V-CLIP still provides low scores, indicating lower data quality. Additionally, extended experiments in Table <ref type="table" target="#tab_4">4</ref> showed that removing V-CLIP led to a performance decline, further demonstrating that augmentation in both directions is meaningful and beneficial for maintaining robust data quality assessment.</p><p>Beaded Hoops. NUK Stack &amp; Store Food Storage Cubes with Lids Reusable Face Masks -Non-Medical 12 Pack Vallejo Metal Color 01: Aluminium 32 ml. A patio with a TV, and seats overlooking an open field. A Women Sitting On A Rock By The Lake Detection Count: 4 Detection Count: 5 Detection Count: 3 Detection Count: 3 Detection Count: 1 Detection Count: 10 A.5.Local Feature Alignment Visualizations. Utilizing the GroundingDINO model, EcoDatum enhances the precision of aligning text descriptions with their corresponding visual content within the dataset. This model effectively in-https ‫ﺑﺨﺎری‬ ‫ﻣﺪل‬ ‫ﮐﺎﻻ‬ ‫ﻧﯿﮏ‬ ‫ﮔﺎزی‬ KN16 ভদরগে িসিল ার ভিত াক উে খােদ Vivaldi -The Masterworks CD 8 (No. 1) -Nicholas McGegan, Various Artists 2014-06-27 16.14.02 420 Outrage 5 DSC07446 Detection Count: 0 Detection Count: 0 Detection Count: 0 Detection Count: 0 Detection Count: 0 Detection Count: 0 Figure 11: Low alignment quality data detected by the Local Cross-Modal Feature Alignment Operator tegrates and cross-verifies textual and visual data, employing a quantitative metric based on the count of bounding boxes whose confidence scores exceed a predefined threshold. This approach is illustrated in Figures 10 and 11, showcasing examples of effective and poor data alignments. Effective Alignments (Figure 10): High detection counts, as seen with items like "NUK Stack &amp; Store Food Storage Cubes" and "Reusable Face Masks," indicate robust correspondence between the text descriptions and visual elements. These examples highlight the model's capability to retain rich, descriptive multimodal content, valuable for training and analytical applications.</p><p>Poor Alignments (Figure <ref type="figure">11</ref>): Conversely, detection counts of zero in examples such as "A Truck on the grass" and a "bicycle parked outside" demonstrate the model's critical role in identifying mismatches. These instances occur when textual descriptions do not visually correspond to the images, or when the described objects are absent from the visual content. Such discrepancies point to insufficient or inaccurate text descriptions, where the text either describes non-existent objects or is too vague.</p><p>By highlighting these mismatches, GroundingDINO ensures the dataset excludes low-quality entries where textual and visual data do not align, thereby maintaining only content with verified and accurate multimodal relationships. This selective curation is essential for developing datasets that are not only diverse but also consistently reliable for training robust multimodal systems.</p><p>A.6.Data Curation Operators Ensemble's Analysis.</p><p>Here, we documented and analyzed the attributes of each Labeling Function and explored the operator weights modeled by the LabelModel during the integration learning phase, as shown in Table <ref type="table" target="#tab_5">5</ref>. It was observed that these operator weights correspond with the earlier single-operator results, indicating a higher reliance on the global alignment operator. This further corroborates the importance of cross-modal global feature alignment in image-text data curation.  The final experiments confirmed that our proposed Eco-Datum method, which currently achieves SOTA on the Dat-aComp benchmark, can identify the most suitable configurations using the proposed search optimization method through this well-constructed candidate pool.</p><p>A.8.Search-based Optimation Validation. To validate the automatic optimization method used for Labeling Functions in EcoDatum, we randomly initialize the weights for three sets of LabelModels. We then conducted repeated experiments by integrating the same unimodal or multimodal operators while applying these random weights, and we retained the previously validated optimal data quantity.</p><p>As demonstrated in Table <ref type="table">7</ref>, the results showed that random initialization of weights not only failed to produce ex- 0.0 0.2 0.4 0.6 0.8 1.0 Operators Inference Scores 0 5 10 15 20 25 Density Language Identification Operator ICC Operator cellent integration performance for the Data Curation task, but also led to negative results in Experiment Group 2. This indicates that without proper integration of the operators, the LabelModel is ineffective for data quality assessment and cannot adequately manage the data. Table 7: Random weights-assigned ensembled curation comparisons, here exp1, exp2, exp3 take three different LabelModel random weights combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9. Scaling Data Experiments</head><p>We conducted additional experiments on EcoDatum scalability on datasets of different sizes. The results demonstrate that EcoDatum consistently enhances performance across different dataset scales, indicating both robustness and scalability of the proposed method. The detailed performance metrics are summarized in the Table <ref type="table" target="#tab_8">8</ref>.</p><p>From these experiments, it is evident that the performance improvement achieved by EcoDatum becomes more pronounced as the dataset size increases. This trend suggests that the method is well-suited for scaling and can deliver greater benefits in larger datasets. In addition, it is worth noting that state-of-the-art visual-language models (VLMs) have achieved remarkable performance using smaller but well-curated datasets. For instance, LLaVA-1.5-HD uses only 558K images for pretraining and 665K for fine-tuning, achieving an 81.8 score on VQAv2. In contrast, Qwen-VL uses 1.4B images for pretraining and 50M for fine-tuning but attains a lower score of 78.8. These examples highlight that high-quality data curation can outweigh the advantages of larger but noisier datasets. This aligns with EcoDatum's central premise: emphasizing data quality over quantity can lead to significant performance improvements.  work, EcoDatum. These visualizations showcase the effectiveness of the EcoDatum in distinguishing and retaining highly aligned image-text pairs, while identifying and excluding low-quality data. The high-quality subset in Figure 15 features images with clear visuals and precise text descriptions that are semantically rich, enhancing the potential of training more effective multimodal models. Conversely, Figure 16 displays low-quality samples characterized by poor image resolution, misaligned texts, or irrelevant content, underscoring the importance of rigorous data filtering to maintain the integrity and utility of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of multimodal curation operators integrating local and global cross-modal feature alignments.</figDesc><graphic coords="4,54.00,54.00,238.50,157.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc>Algorithm 1: Search Optimization for LFs 1: Input: 2: Raw dataset D raw , Tiny-labeled dataset D tiny , Operator Op, Evaluation metrics M , Candidate LF Combinations LF sCombs, M * 6: for each LF s ∈ LF sCombs do 7: Convert weak supervision label matrix L M from LF s LabelModel on D tiny 10: if M (LF s) &gt; M * then 11: Update M * ← M (LF s) and LF s * ← LF s 12:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Composite Metric Validation with Repeated Experimental Downtasks Evaluations. The positive correlation indicates its capability to guide the tuning of the process.</figDesc><graphic coords="7,321.89,253.28,233.73,170.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc>Exploring the balance between quantity and performance. Approximately the top 40% samples by the Eco-Datum quality score after deduplication (around 3.5M) provides the best balance between quality and quantity,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Vary the filtered dataset quantity and compare the results of repeated experiments to find an optimal balance at 3.5M samples under EcoDatum curation sorting.</figDesc><graphic coords="10,319.50,54.00,238.07,272.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Quality Guided Deduplication Samples: Among each duplicated group, only the finely aligned image-text pairs are kept and left others to improve the overall distribution with maintaining cross-modal quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>•</head><figDesc>Enhances Model Training: High-quality, well-aligned data leads to more accurate and robust models.• Maintains Dataset Diversity: By selectively keeping the best-aligned pairs, the method supports a diverse and balanced dataset, which is critical for generalizability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Hard Bad Cases Mined by V-CLIP Operator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: High alignment quality data detected by the Local Cross-Modal Feature Alignment Operator</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Multimodal Operators Inf. Scores' Distributions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><figDesc>Figure 13: Unimodal(Visual-based) Operators Inf. Scores' Distributions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Unimodal(Textual-based) Operators Inf. Scores' Distributions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figures</head><figDesc>Figures 15 and 16 illustrate the significant differences in quality between selected high-quality data samples and filtered low-quality data samples by our data curation frame-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="3,54.00,54.00,503.99,118.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison between our method, the Datacomp baseline, and other participants' approaches.</figDesc><table><row><cell></cell><cell>No Filtering</cell><cell>LAION</cell><cell>Datacomp</cell><cell>CLIP</cell><cell>HYPE</cell><cell>LINE</cell><cell>T-MARS</cell><cell>WS</cell><cell>Ours</cell></row><row><cell>Dataset Size</cell><cell>12.8M</cell><cell>1.3M</cell><cell>3M</cell><cell>3.8M</cell><cell>2.3M</cell><cell>4.5M</cell><cell>2.3M</cell><cell>4.1M</cell><cell>3.5M</cell></row><row><cell>Avg. Perf.</cell><cell>0.132</cell><cell>0.133</cell><cell>0.142</cell><cell>0.173</cell><cell>0.176</cell><cell>0.177</cell><cell>0.180</cell><cell>0.180</cell><cell>0.182</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The "No Filtering" condition acts as the control group."Random Deduplication" utilizes a stochastic method to eliminate duplicates, indicating that even indiscriminate reductions can improve model performance by balancing feature distribution.</figDesc><table><row><cell>Methods</cell><cell cols="2">Dataset Size Avg. Perf.</cell></row><row><cell>No Filtering</cell><cell>12.8M</cell><cell>0.132</cell></row><row><cell>Random Dedup.</cell><cell>8.8M</cell><cell>0.145</cell></row><row><cell>Quality-Guided Dedup.(QGD)</cell><cell>8.8M</cell><cell>0.147</cell></row><row><cell>QGD+Ens.(Uni.)</cell><cell>3.5M</cell><cell>0.154</cell></row><row><cell>QGD+Ens.(Mul.)</cell><cell>3.5M</cell><cell>0.164</cell></row><row><cell>QGD+Ens.(Uni.&amp;Global-Mul.)</cell><cell>3.5M</cell><cell>0.168</cell></row><row><cell>QGD+Ens.(Uni.&amp;Local-Mul.)</cell><cell>3.5M</cell><cell>0.155</cell></row><row><cell>Best Perf.</cell><cell></cell><cell></cell></row><row><cell>QGD+Ens.(Uni.&amp;Mul.)</cell><cell>3.5M</cell><cell>0.182</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of single operator curation.</figDesc><table><row><cell>No Filtering</cell><cell>12.8M</cell><cell>0.132</cell></row><row><cell>Image-Blurry</cell><cell>10.8M</cell><cell>0.138</cell></row><row><cell>Image-Geometric</cell><cell>10.7M</cell><cell>0.144</cell></row><row><cell>Text-Language Id.</cell><cell>5.3M</cell><cell>0.150</cell></row><row><cell>Text-ICC</cell><cell>3.9M</cell><cell>0.147</cell></row><row><cell>CLIP</cell><cell>7.5M</cell><cell>0.146</cell></row><row><cell>H-CLIP</cell><cell>4.3M</cell><cell>0.160</cell></row><row><cell>V-CLIP</cell><cell>7.2M</cell><cell>0.154</cell></row><row><cell>GroundingDINO</cell><cell>6.2M</cell><cell>0.141</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Exp. (Without H-CLIP) vs. Best Perf. (With H-CLIP)</figDesc><table><row><cell></cell><cell cols="4">Blurry Geometry Language</cell><cell>ICC</cell><cell cols="4">CLIP H-CLIP V-CLIP GDINO Perf.</cell></row><row><cell cols="2">Control Group. 0.5593</cell><cell>0.5341</cell><cell cols="2">0.8438</cell><cell cols="2">0.7467 0.7334</cell><cell>-</cell><cell>0.5875</cell><cell>0.7467</cell><cell>0.169</cell></row><row><cell>Best Perf.</cell><cell>0.7564</cell><cell>0.6186</cell><cell cols="2">0.6595</cell><cell cols="2">0.5832 0.8078</cell><cell>0.9154</cell><cell>0.8775</cell><cell>0.6329</cell><cell>0.182</cell></row><row><cell></cell><cell cols="2">Operators</cell><cell></cell><cell cols="5">Coverage Overlaps Conflicts Weights</cell></row><row><cell></cell><cell>Blurry</cell><cell></cell><cell></cell><cell>0.003</cell><cell></cell><cell>0.003</cell><cell>0.002</cell><cell>0.756</cell></row><row><cell></cell><cell cols="2">Geometry</cell><cell></cell><cell>0.014</cell><cell></cell><cell>0.014</cell><cell>0.011</cell><cell>0.619</cell></row><row><cell></cell><cell cols="3">Language Identification</cell><cell>0.532</cell><cell></cell><cell>0.524</cell><cell>0.362</cell><cell>0.660</cell></row><row><cell></cell><cell>ICC</cell><cell></cell><cell></cell><cell>0.345</cell><cell></cell><cell>0.343</cell><cell>0.239</cell><cell>0.583</cell></row><row><cell></cell><cell>CLIP</cell><cell></cell><cell></cell><cell>0.739</cell><cell></cell><cell>0.733</cell><cell>0.471</cell><cell>0.808</cell></row><row><cell></cell><cell>H-CLIP</cell><cell></cell><cell></cell><cell>0.754</cell><cell></cell><cell>0.738</cell><cell>0.410</cell><cell>0.915</cell></row><row><cell></cell><cell>V-CLIP</cell><cell></cell><cell></cell><cell>0.658</cell><cell></cell><cell>0.657</cell><cell>0.409</cell><cell>0.878</cell></row><row><cell></cell><cell cols="2">GroundingDINO</cell><cell></cell><cell>0.423</cell><cell></cell><cell>0.421</cell><cell>0.312</cell><cell>0.633</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ensembled Operators Attributes and Analyzed WeightsA.7.Search-based Optimization Implementation Details.To better configure the candidate Labeling Functions, we first analyze the inference score distribution of each operator across the entire dataset, as shown in Figure12, Figure13and Figure14. We then introduced the TopK values to construct candidate Labeling Functions to assist in converting output scores into weak supervision labels, as shown in Table6.</figDesc><table><row><cell></cell><cell>8 10</cell><cell></cell><cell></cell><cell cols="2">CLIP V-CLIP H-CLIP GroundingDINO</cell></row><row><cell>Density</cell><cell>4 6</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0.0</cell><cell>0.2 Operators Inference Scores 0.4 0.6</cell><cell>0.8</cell><cell>1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Single Operators Inference TopK Scores</figDesc><table><row><cell></cell><cell cols="8">TopK Global Feature Alignment ICC Operator Local Feature Alignment</cell></row><row><cell></cell><cell>Top20</cell><cell>0.271</cell><cell></cell><cell></cell><cell>0.064</cell><cell></cell><cell>0.225</cell><cell></cell></row><row><cell></cell><cell>Top30</cell><cell>0.252</cell><cell></cell><cell></cell><cell>0.041</cell><cell></cell><cell>0.206</cell><cell></cell></row><row><cell></cell><cell>Top40</cell><cell>0.235</cell><cell></cell><cell></cell><cell>0.035</cell><cell></cell><cell>0.192</cell><cell></cell></row><row><cell></cell><cell>Top50</cell><cell>0.220</cell><cell></cell><cell></cell><cell>0.023</cell><cell></cell><cell>0.180</cell><cell></cell></row><row><cell></cell><cell>Top60</cell><cell>0.211</cell><cell></cell><cell></cell><cell>0.017</cell><cell></cell><cell>0.169</cell><cell></cell></row><row><cell></cell><cell>Top70</cell><cell>0.192</cell><cell></cell><cell></cell><cell>0.013</cell><cell></cell><cell>0.158</cell><cell></cell></row><row><cell></cell><cell>Top80</cell><cell>0.187</cell><cell></cell><cell></cell><cell>0.010</cell><cell></cell><cell>0.146</cell><cell></cell></row><row><cell>Settings</cell><cell cols="3">Blurry Geometry Language</cell><cell>ICC</cell><cell cols="5">CLIP H-CLIP V-CLIP GDINO Perf.</cell></row><row><cell>Exp1</cell><cell>0.5924</cell><cell>0.1241</cell><cell>0.6004</cell><cell cols="2">0.9877 0.3738</cell><cell>0.5904</cell><cell>0.9737</cell><cell>0.7416</cell><cell>0.157</cell></row><row><cell>Exp2</cell><cell>0.8402</cell><cell>0.1481</cell><cell>0.3681</cell><cell cols="2">0.0736 0.2766</cell><cell>0.8365</cell><cell>0.9522</cell><cell>0.0082</cell><cell>0.141</cell></row><row><cell>Exp3</cell><cell>0.0827</cell><cell>0.7436</cell><cell>0.4301</cell><cell cols="2">0.8606 0.7251</cell><cell>0.8246</cell><cell>0.8109</cell><cell>0.7574</cell><cell>0.161</cell></row><row><cell cols="2">Best Perf. 0.7564</cell><cell>0.6186</cell><cell>0.6595</cell><cell cols="2">0.5832 0.8078</cell><cell>0.9154</cell><cell>0.8775</cell><cell>0.6329</cell><cell>0.182</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Performance of EcoDatum Across Different Dataset SizesA.10.Selected or Filtered Subsets Visualization.</figDesc><table><row><cell>Size</cell><cell cols="3">0.128m 1.28m 12.8m</cell></row><row><cell>No-filtering</cell><cell>0.073</cell><cell>0.088</cell><cell>0.132</cell></row><row><cell>EcoDatum</cell><cell>0.075</cell><cell>0.104</cell><cell>0.182</cell></row><row><cell>Improvement</cell><cell>0.002</cell><cell>0.016</cell><cell>0.050</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.datacomp.ai/dcclip/leaderboard.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported in part by <rs type="funder">China NSF</rs> grant No. <rs type="grantNumber">62202297</rs> and in part by the <rs type="institution">Open Project Program of the Laboratory of Pinghu</rs>. The opinions, findings, conclusions, and recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies or the government.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_GV4nbWW">
					<idno type="grant-number">62202297</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semdedup: Data-efficient learning at web-scale through semantic deduplication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.09540</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="23716" to="23736" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Snorkel drybell: A case study in deploying weak supervision at industrial scale</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Alborzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Management of Data</title>
		<meeting>the 2019 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="362" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nürnberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Köhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9368" to="9377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bagging predictors. Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<idno>ArXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data-juicer: A one-stop data processing system for large language models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion of the 2024 International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="120" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An overview of perceptual hashing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Online Trust and Safety</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer and system sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DataComp: In search of the next generation of multimodal datasets</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hayase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Orgad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Entezari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanujan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Saukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Datacomp: In search of the next generation of multimodal datasets</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hayase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smyrnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D L</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<title level="m">Training compute-optimal large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Adila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.12225</idno>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2024. 2021</date>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Multimodal Data Curation via Object Detection and Filter Ensembles</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<idno>arXiv:2404.17507</idno>
		<title level="m">HYPE: Hyperbolic Entailment Filtering for Underspecified Images and Texts</title>
		<imprint>
			<date type="published" when="2016">2016. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Fasttext. zip: Compressing text classification models</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Blip: Bootstrapping language-image pre-training for unified visionlanguage understanding and generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12888" to="12900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014: 13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09-06">2014. September 6-12, 2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Grounding dino: Marrying dino with grounded pre-training for open-set object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2025">2025</date>
			<biblScope unit="page" from="38" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Grounding dino: Marrying dino with grounded pre-training for open-set object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.05499</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Maini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.03132</idno>
		<title level="m">T-mars: Improving visual representations by circumventing text feature learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Dataset inference: Ownership resolution in machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yaghini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10706</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prioritized training on points that are learnable, worth learning, and not yet learnt</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mindermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Brauner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Razzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Höltgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Morisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15630" to="15649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving multimodal datasets with image captioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Gadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Filtering, distillation, and hard negatives for vision-language pre-training</title>
		<author>
			<persName><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="6967" to="6977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Snorkel: Rapid training data creation with weak supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ehrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB endowment. International conference on very large data bases</title>
		<meeting>the VLDB endowment. International conference on very large data bases</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Laion-5b: An open largescale dataset for training next generation image-text models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cherti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25278" to="25294" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Laion-400m: Open dataset of clip-filtered 400 million image-text pairs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vencu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Beaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaczmarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Coombes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.02114</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with deep neural networks: A survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8135" to="8153" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning-based collaborative filtering recommender systems: A comprehensive and systematic review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torkashvand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Jameii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">35</biblScope>
			<biblScope unit="page" from="24783" to="24827" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Yanuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.01306</idno>
		<title level="m">ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Yokoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.14581</idno>
		<title level="m">Leveraging Image-Text Similarity and Caption Modification for the DataComp Challenge: Filtering Track and BYOD Track</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey on unsupervised outlier detection in high-dimensional numerical data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zimek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.15954</idno>
	</analytic>
	<monogr>
		<title level="m">The devil is in the details: A deep dive into the rabbit hole of data filtering</title>
		<imprint>
			<date type="published" when="2012">2023. 2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="363" to="387" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
